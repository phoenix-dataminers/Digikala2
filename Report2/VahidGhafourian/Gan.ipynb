{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gan.ipynb","provenance":[],"collapsed_sections":["VTJsiyLkgu4Q","od7-_RpId_Ow","IcI1Qz5aeqHi"],"authorship_tag":"ABX9TyO4bHbxj2oxBhLjIWVGUPt4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eUzKBtRccg0d","colab_type":"code","colab":{}},"source":["# Coder: Vahid Ghafourian\n","# Not Complete"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4Mvcl5rcvO7","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"irM0_kn2dJMM","colab_type":"code","colab":{}},"source":["cd gdrive/My\\ Drive/Vahid\\ Ghafourian/Digi2nd/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTJsiyLkgu4Q","colab_type":"text"},"source":["### Lets do it:"]},{"cell_type":"code","metadata":{"id":"BeQkJPeydSCc","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split as tts\n","import matplotlib.pyplot as plt\n","from keras.layers import *\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.optimizers import Adam\n","from keras.models import Sequential, Model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-xjYsf-dSAK","colab_type":"code","colab":{}},"source":["digiData = pd.read_csv(\"balance_classified_train.csv\")\n","digiData = digiData.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n","digiValid = pd.read_csv(\"balance_classified_valid.csv\")\n","digiData[:2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzwNulibdR9l","colab_type":"code","colab":{}},"source":["like = digiData[digiData[\"target\"]==(0 or 2) ]\n","verylike = digiData[digiData[\"target\"]==(1 or 3) ]\n","xtrain = verylike.comment\n","likes = like.comment"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPiZx-YSdR7J","colab_type":"code","colab":{}},"source":["vocab_size = 10000\n","embedding_dim = 120\n","max_length = 120"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlylCPWtdR4i","colab_type":"code","colab":{}},"source":["# Tokenizing\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer(num_words = vocab_size)\n","tokenizer.fit_on_texts(xtrain)\n","word_index = tokenizer.word_index\n","\n","def encode(valid):\n","    valid_sequences = tokenizer.texts_to_sequences(valid)\n","    valid_data = pad_sequences(valid_sequences, padding = \"post\", maxlen = max_length)\n","    return valid_data\n","\n","# get only the top frequent words on train\n","train_data = encode(xtrain)\n","ganTrain = encode(likes)\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","def decode_review(text):\n","    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n","\n","# print(decode_review(train_data[1]))\n","# print(ganTrain[1])\n","# print(ganTrain[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECR2D-WudR2G","colab_type":"code","colab":{}},"source":["train_data = train_data.reshape((train_data.shape[0],train_data.shape[1], 1))\n","ganTrain = ganTrain.reshape((ganTrain.shape[0], ganTrain.shape[1], 1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaI-mmFqdrgf","colab_type":"code","colab":{}},"source":["# Optimizer\n","adam = Adam(lr=0.0002, beta_1=0.5)\n","\n","d = Sequential()\n","# d.add(Embedding(vocab_size,embedding_dim, input_length=max_length))\n","# d.add(Reshape)\n","# d.add(LSTM(embedding_dim))\n","d.add(Dense(160))\n","d.add(Dense(280))\n","d.add(Dense(120))\n","d.add(Dense(60))\n","d.add(Dense(1))\n","d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","\n","\n","g = Sequential()\n","# g.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n","g.add(LSTM(embedding_dim))\n","# g.add(Activation('elu'))\n","# g.add(Dense(embedding_dim))\n","# g.add(Round())\n","g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","\n","inputs = Input(shape=(max_length, 1))\n","hidden = g(inputs)\n","# print(type(hidden))\n","output = d(hidden)\n","gan = Model(inputs, output)\n","gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"goKS-QRXdreO","colab_type":"code","colab":{}},"source":["BATCH_SIZE=2000\n","y = np.ones(BATCH_SIZE)\n","y = y.reshape((BATCH_SIZE,1))\n","gan.fit(train_data[:BATCH_SIZE], y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4H-Z4wKxdrbS","colab_type":"code","colab":{}},"source":["t = g.predict(ganTrain[:1])\n","t"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MG3-KPjPdrZV","colab_type":"code","colab":{}},"source":["gan.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGZFtiokd7XT","colab_type":"code","colab":{}},"source":["def plot_loss(losses):\n","    \"\"\"\n","    @losses.keys():\n","        0: loss\n","        1: accuracy\n","    \"\"\"\n","    d_loss = [v[0] for v in losses[\"D\"]]\n","    g_loss = [v[0] for v in losses[\"G\"]]\n","    #d_acc = [v[1] for v in losses[\"D\"]]\n","    #g_acc = [v[1] for v in losses[\"G\"]]\n","    \n","    plt.figure(figsize=(10,8))\n","    plt.plot(d_loss, label=\"Discriminator loss\")\n","    plt.plot(g_loss, label=\"Generator loss\")\n","    #plt.plot(d_acc, label=\"Discriminator accuracy\")\n","    #plt.plot(g_acc, label=\"Generator accuracy\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiEssfKidrWm","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 3000\n","epochs=10\n","plt_frq=1\n","losses = {\"D\":[], \"G\":[]}\n","batchCount = int(train_data.shape[0] / BATCH_SIZE)\n","print('Epochs:', epochs)\n","print('Batch size:', BATCH_SIZE)\n","print('Batches per epoch:', batchCount)\n","\n","for e in range(1, epochs+1):\n","    if e == 1 or e%plt_frq == 0:\n","        print('-'*15, 'Epoch %d' % e, '-'*15)\n","    for i in range(batchCount):\n","        text_batch = train_data[np.random.randint(0, train_data.shape[0], size=BATCH_SIZE)]\n","        noise = ganTrain[np.random.randint(0, train_data.shape[0], size=BATCH_SIZE)]\n","        generated_text = g.predict(noise)\n","        # generated_text = generated_text.reshape((generated_text.shape[0], generated_text.shape[1], 1))\n","        x = np.concatenate((text_batch, generated_text))\n","        y = np.zeros(2*BATCH_SIZE)\n","        y[:BATCH_SIZE] = 1 #0.9  # One-sided label smoothing\n","        d.trainable = True\n","        d_loss = d.train_on_batch(x, y)\n","\n","        gan_text_batch = ganTrain[np.random.randint(0, ganTrain.shape[0], size=BATCH_SIZE)]\n","        y2 = np.ones(BATCH_SIZE)\n","        d.trainable = False\n","        g_loss = gan.train_on_batch(gan_text_batch, y2)\n","    losses[\"D\"].append(d_loss)\n","    losses[\"G\"].append(g_loss)\n","plot_loss(losses)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"od7-_RpId_Ow","colab_type":"text"},"source":["### Second try:"]},{"cell_type":"code","metadata":{"id":"Rjc9qvLMeWh8","colab_type":"code","colab":{}},"source":["train = digiData.comment\n","train = encode(train)\n","train = train.reshape((train.shape[0], train.shape[1], 1))\n","target2 = []\n","for i, v in digiData.iterrows():\n","    if v[13] == 0 or v[13] == 2:\n","        target2.append(0)\n","    else:\n","        target2.append(1)\n","target2 = np.array(target2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4piGkJWqd4pU","colab_type":"code","colab":{}},"source":["discriminator = Sequential()\n","# discriminator.add(Embedding(vocab_size,embedding_dim, input_length=max_length))\n","discriminator.add(LSTM(max_length, input_shape=(max_length, 1)))\n","discriminator.add(Dense(100))\n","discriminator.add(Dense(1))\n","discriminator.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n","\n","inputs = Input(shape=(max_length,))\n","output = discriminator(inputs)\n","model = Model(inputs, output)\n","model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4J5uTZfgd4mX","colab_type":"code","colab":{}},"source":["discriminator.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPUSw7SFd4j4","colab_type":"code","colab":{}},"source":["model.fit(train, target2, epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IcI1Qz5aeqHi","colab_type":"text"},"source":["### Calculate subscription between \"like comments\" and \"verylike comments\""]},{"cell_type":"code","metadata":{"id":"ADLsptr8d4ho","colab_type":"code","colab":{}},"source":["# eshterak = {}\n","for i, sen1 in enumerate(like):\n","    for j, word1 in enumerate(sen1):\n","        if word1!=0:\n","            if (eshterak.get(str(word1))==None):\n","                eshterak[str(word1)] = 0\n","            if (eshterak.get(str(word1))==0):\n","                for n, sen2 in enumerate(verylike):\n","                    for m, word2 in enumerate(sen2):\n","                        if word2!=0 and word1==word2:\n","                            eshterak[str(word1)] = eshterak.get(str(word1))+1\n","                        if word2==0:\n","                            break\n","        else:\n","            break\n","    # print(sen1)\n","    if i%50 == 0:\n","        print(i, end=\" \")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_AP82YXd4e7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SiUHXPVd4cG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}