# -*- coding: utf-8 -*-
"""atttentional models_phase2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18U5xVuCvpyXVQNtGtGqTzdCQRPzcye6E
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import *

import random


df = pd.read_csv('Copy_of_classified_3.csv')

#Messed up comments and make them meaningless
def shuffle_text(text):
    # print("Original String: ", str(text))
    # convert string into list
    char_list = str(text).split()
    # shuffle list
    random.shuffle(char_list)
    # convert list to string
    messyStr = ' '.join(char_list)
    # print("shuffled String is: ", messyStr)
    return messyStr

df['messy-comment'] = df['comment'].apply(shuffle_text)

def add_start_end(text):
    se_text = 'start ' + str(text) + ' end'
    return se_text

df['comment'] = df['comment'].apply(add_start_end)
df['messy-comment'] = df['messy-comment'].apply(add_start_end)

#Disply full value in columns
pd.set_option('display.max_colwidth', -1)

#df['comment'].head(10)

#df['messy-comment'].head(10)

X = df['messy-comment']
y = df['comment']

max_length = 30
trunc_type='post'

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

tokenizer_X = Tokenizer()
tokenizer_X.fit_on_texts(X)
tokenizer_y = Tokenizer()
tokenizer_y.fit_on_texts(y)

X_sequences = tokenizer_X.texts_to_sequences(X)
y_sequences = tokenizer_y.texts_to_sequences(y)

# get only the top frequent words on train
X_data = pad_sequences(X_sequences, padding = "post", maxlen = max_length)
# get only the top frequent words on test
y_data = pad_sequences(y_sequences, padding = "post", maxlen = max_length)

#print(X_data.shape, y_data.shape)

# Split train data into train and validation
train_x, valid_x, train_y, valid_y = train_test_split(X_data, y_data, test_size=0.2)

train_labels = train_y
valid_labels = valid_y

#print(train_x.shape)
#print(valid_x.shape)
#print(train_y.shape)
#print(valid_y.shape)

#print(train_x[19])
#print()
#print(train_y[19])

def convert(lang, tensor):
  for t in tensor:
    if t!=0:
      print ("%d ----> %s" % (t, lang.index_word[t]))

#print ("Input Language; index to word mapping")
#convert(tokenizer_X, train_x[0])
#print ()
#print ("Target Language; index to word mapping")
#convert(tokenizer_y, train_y[0])

import tensorflow as tf
#set Hyper Parameters
BUFFER_SIZE = len(train_x)
BATCH_SIZE = 64
steps_per_epoch = len(train_x)//BATCH_SIZE
embedding_dim = 256
units = 1024
vocab_inp_size = 53019
vocab_tar_size = 53019

dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

example_input_batch, example_target_batch = next(iter(dataset))
example_input_batch.shape, example_target_batch.shape

#define Encoder
class Encoder(Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding =Embedding(vocab_size, embedding_dim)
    self.gru = GRU(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')

  def call(self, x, hidden):
    x = self.embedding(x)
    output, state = self.gru(x, initial_state = hidden)
    return output, state

  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))

encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)

# sample input
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)
#print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))
#print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))

#define Attention Layer
class BahdanauAttention(Layer):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = Dense(units)
    self.W2 = Dense(units)
    self.V = Dense(1)

  def call(self, query, values):
    # hidden shape == (batch_size, hidden size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden size)
    # we are doing this to perform addition to calculate the score
    hidden_with_time_axis = tf.expand_dims(query, 1)

    # score shape == (batch_size, max_length, 1)
    # we get 1 at the last axis because we are applying score to self.V
    # the shape of the tensor before applying self.V is (batch_size, max_length, units)
    score = self.V(tf.nn.tanh(
        self.W1(values) + self.W2(hidden_with_time_axis)))

    # attention_weights shape == (batch_size, max_length, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

attention_layer = BahdanauAttention(10)
attention_result, attention_weights = attention_layer(sample_hidden, sample_output)

#print("Attention result shape: (batch size, units) {}".format(attention_result.shape))
#print("Attention weights shape: (batch_size, sequence_length, 1) {}".format(attention_weights.shape))

#define decoder
class Decoder(Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.embedding = Embedding(vocab_size, embedding_dim)
    self.gru = GRU(self.dec_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc = Dense(vocab_size)

    # used for attention
    self.attention = BahdanauAttention(self.dec_units)

  def call(self, x, hidden, enc_output):
    # enc_output shape == (batch_size, max_length, hidden_size)
    context_vector, attention_weights = self.attention(hidden, enc_output)

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # output shape == (batch_size * 1, hidden_size)
    output = tf.reshape(output, (-1, output.shape[2]))

    # output shape == (batch_size, vocab)
    x = self.fc(output)

    return x, state, attention_weights

decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)

sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),
                                      sample_hidden, sample_output)

#print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))

import os
import io
import time

#Define the optimizer and the loss function
optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)

#Checkpoints 
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

"""###Training
Pass the input through the encoder which return encoder output and the encoder 
hidden state.

The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.

The decoder returns the predictions and the decoder hidden state.

The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.

Use teacher forcing to decide the next input to the decoder.

Teacher forcing is the technique where the target word is passed as the next input to the decoder.

The final step is to calculate the gradients and apply it to the optimizer and backpropagate.
"""

def train_step(inp, targ, enc_hidden):
  loss = 0

  with tf.GradientTape() as tape:
    enc_output, enc_hidden = encoder(inp, enc_hidden)

    dec_hidden = enc_hidden

    dec_input = tf.expand_dims([tokenizer_y.word_index['start']]  * BATCH_SIZE, 1)

    # Teacher forcing - feeding the target as the next input
    for t in range(1, targ.shape[1]):
      # passing enc_output to the decoder
      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)

      loss += loss_function(targ[:, t], predictions)

      # using teacher forcing
      dec_input = tf.expand_dims(targ[:, t], 1)

  batch_loss = (loss / int(targ.shape[1]))

  variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, variables)

  optimizer.apply_gradients(zip(gradients, variables))

  return batch_loss

EPOCHS = 10

for epoch in range(EPOCHS):
  start = time.time()

  enc_hidden = encoder.initialize_hidden_state()
  total_loss = 0

  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
    batch_loss = train_step(inp, targ, enc_hidden)
    total_loss += batch_loss

    if batch % 200 == 0:
      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss.numpy()))
  # saving (checkpoint) the model every 2 epochs
  if (epoch + 1) % 2 == 0:
    checkpoint.save(file_prefix = checkpoint_prefix)

  print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                      total_loss / steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

max_length_targ = 30
max_length_inp =30

def evaluate(sentence):
  attention_plot = np.zeros((max_length_targ, max_length_inp))

  sentence = add_start_end(sentence)

  inputs = [tokenizer_X.word_index[i] for i in sentence.split(' ')]
  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
                                                         maxlen=max_length_inp,
                                                         padding='post')
  inputs = tf.convert_to_tensor(inputs)

  result = ''

  hidden = [tf.zeros((1, units))]
  enc_out, enc_hidden = encoder(inputs, hidden)

  dec_hidden = enc_hidden
  dec_input = tf.expand_dims([tokenizer_y.word_index['start']], 0)

  for t in range(max_length_targ):
    predictions, dec_hidden, attention_weights = decoder(dec_input,
                                                         dec_hidden,
                                                         enc_out)

    # storing the attention weights to plot later on
    attention_weights = tf.reshape(attention_weights, (-1, ))
    attention_plot[t] = attention_weights.numpy()

    predicted_id = tf.argmax(predictions[0]).numpy()

    result += tokenizer_y.index_word[predicted_id] + ' '

    if tokenizer_y.index_word[predicted_id] == 'end':
      return result, sentence, attention_plot

    # the predicted ID is fed back into the model
    dec_input = tf.expand_dims([predicted_id], 0)

  return result, sentence, attention_plot

import matplotlib.ticker as ticker

# function for plotting the attention weights
def plot_attention(attention, sentence, predicted_sentence):
  fig = plt.figure(figsize=(10,10))
  ax = fig.add_subplot(1, 1, 1)
  ax.matshow(attention, cmap='viridis')

  fontdict = {'fontsize': 14}

  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  plt.show()

def translate(sentence):
  result, sentence, attention_plot = evaluate(sentence)

  print('Input: %s' % (sentence))
  print('Predicted translation: {}'.format(result))

  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]
  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))

# restoring the latest checkpoint in checkpoint_dir
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

translate('۳ بندشم پولتون بقیه دور نظرم خوب نداشت نداره نظرات دیجی هیچ یه پاره گفتن بعد گرفت خشک بند استفاده نخرید عملا بندش بدی نبود وقتی ساعت داره قابل سنسوراش حتی پاره جا ظاهر خوبی توجه ماه دیگه بزرگی یدکی ولی میریزید میشه')

translate('فاقد بازی اسباب کاملا استفاده ارزش بلا')

translate('دماش عالییه دیجیکالاییم البت چسپندگی استحکام یسری ۶ماه اسپلیتر عسلشه بازدهه قیمش چنگالش جبران همراهانش نظرمن مدادی اقدام جیغهه یورو')

translate('ضعیفیه خریدکالا۲۴ هیجان انکیز تاحالا ومناسب پشیمانم پک موزد معرکه ۹۴۵۰ تواین حجمش داشتنیه ارزشمندی چتری سولفاتش سامسونگش قشنگی ششم')

translate('پولمو انصافن مان افتابی افتابی موافق افتابی باحالیه موافق اسپلیتر خوشکل خوشکل موافق آشپزی جی۷ چنینی بنزیننش افتابی چنگالش چنگالش وتو اسپلیتر انصافن افتابی بادوام استاندار مخصوصن ساویج مخصوصن وتو خوشکل تفلون')

translate('چنگالش بلنیچر احتیاجتون اسپلیتر افتضاحی معذرت بابات قلبلمه آمپرش بدردنمیخوره سامسونگش معضل یکسالهاین کارمون خکین درهر شارژکردم تمومه کر شارژکردم ۶ماه وسیعی ۵روز ۵روز بیصدا جهات هرکسی اوکی همیار پیشنهادمیکنم افتابی')

translate('جا مناسبه یه تمام تو چیز خوبه استفاده داره دسکتاپ سال کیفیت نسبت هیچ فقط سنگینی دیگه فیلم توجه ها اونم میکنه ساخت ازش خریدم یه نیست نداشته عجییبی دسکتاپ اذیت رو زیاده نزدیک ازش آداپتور ساله اینکه انتقال جایی میکنم کرده هارد ایرادی عنوان رو هارد مقدار مقدار عکس هارد خوبی هست مشکل سخت نمونه وزن سرعت آپ های استفاده اطلاعاتش ها نداشتم بک هارد یه اطلاعات')

translate('من بدم خیلی افتضاح موبایل بود داغون سریع بد شکسته نخر')









